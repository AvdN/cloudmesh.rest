\documentclass[10pt]{article} 

\input{styles/simple-format}
\makeatother

\setcounter{secnumdepth}{6}
\setcounter{tocdepth}{6}


%\titleformat{\section}
%{\normalfont\Large\sffamily\bfseries}{}{0em}{\colorbox{black}{\parbox{\dimexpr\textwidth-2\fboxsep\relax}{\textcolor{white}{\thesecti%on\quad#1}}}}
%\titleformat{name=\section,numberless}
%{\normalfont\Large\sffamily\bfseries}{}{0em}{\colorbox{black}{\parbox{\dimexpr\textwidth-2\fboxsep\relax}{\textcolor{white}{#1}}}}


%\renewcommand*{\journallongtype}{NIST Big Data Interoperability Framework: Volume 8, Reference Architecture Interface}

\title{NIST Big Data Interoperability Framework: Volume 8, Reference Architecture Interface}

\author{Gregor von Laszewski,Fugang Wang, Badi Abdhul-Wahid, Wo L. Chang}

%  \affil[1]{School of Informatics and Computing, Bloomington, IN 47408, U.S.A.} 
%  \affil[2]{National INstitute of Standards, 100 Bureau Drive, Gaithersburg, MD 20899} 
%  \affil[*]{Corresponding author: laszewski@gmal.com} 

\date{Draft v0.0.1, \today} 

%  \ociscodes{Cloudmesh, REST, NIST}

%  \doi{\url{https://github.com/cloudmesh/rest/tree/master/docs}} 


\newcommand{\ABSTRACT}{
This document summarizes a number of objects that are instrumental for
the interaction with Clouds, Containers, and HPC systems to manage
virtual clusters. 

Big Data is a term used to describe the large amount of data in the
networked, digitized, sensor-laden, information-driven world. While
opportunities exist with Big Data, the data can overwhelm traditional
technical approaches, and the growth of data is outpacing scientific
and technological advances in data analytics. To advance progress in
Big Data, the NIST Big Data Public Working Group (NBD-PWG) is working
to develop consensus on important fundamental concepts related to Big
Data. The results are reported in the NIST Big Data Interoperability
Framework series of volumes. This volume, Volume 6, summarizes the
work performed by the NBD-PWG to characterize Big Data from an
architecture perspective, presents the NIST Big Data Reference
Architecture (NBDRA) conceptual model, and discusses the components
and fabrics of the NBDRA.

}


\begin{document}

\flushbottom % Makes all text pages the same height

\maketitle % Print the title and abstract box

\tableofcontents % Print the contents section
\maketitle


%\input{nist}

\section*{\hfil \hspace{4cm} Abstract \hfil}
\ABSTRACT

\section*{\hfil  \hspace{4cm} Keywords \hfil}
Application Provider; Big Data; Big Data characteristics; Data Consumer; Data Provider; Framework Provider; Management Fabric; reference architecture; Security and Privacy Fabric; System Orchestrator; use cases.

\cite{*}

\bibliography{references}


\newpage

\linenumbers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{NBDRA Interface Requirements}

The Volume 6 Reference Architecture document provides a list of comprehensive high-level reference architecture requirements and introduces the NIST Big Data Reference Architecture (NBDRA) (see Figure \ref{F:architecture}). To enable interoperability between the NBDRA components, a list of well-defined NBDRA interface is needed.  To introduce them, we will follow the NBDRA and focus on interfaces that allow us to bootstrap the NBDRA. Each section will introduce an Interface while documenting the requirement as well as a simple specification addressing the immediate interface needs. We expect that this document will grow with the help of contributions from the community to achieve a comprehensive set of interfaces that will be usable for the implementation of Big Data Architectures. Validation of this approach can be achieved while applying it to the use cases that have been gathered in Volume 3. These use cases have considerably contributed towards the design of the NBDRA. Hence our expectation is that (a) the interfaces can be used to help implementing a big data architecture for a specific use case, and (b) the proper implementation can validate the NBDRA. Through this approach, we can facilitate subsequent analysis and comparison of the use cases. 


\subsection{High Level Requirements of the Interface Approach}

Next, we focus on the high-level requirements of the interface approach as depicted in \ref{F:architecture}

\begin{figure*}[h]\centering
\includegraphics[width=1.1\textwidth]{images/vol8-diagrams}
\caption{NIST Big Data Reference Architecture (NBDRA)}
\label{F:architecture}
\end{figure*}


\subsubsection{Technology and Vendor Agnostic}

Due to the many different tools, services, and infrastructures available in the general area of big data an interface ought to be as vendor independent as possible, while at the same time be able to leverage best practices. As such we need to provide a methodology that allows extension of interfaces to adapt and leverage existing approaches, but also allows the interfaces to provide merit in easy specifications that assist the formulation and definition of the NBDRA. 

\subsubsection{Support of Plug-In Compute Infrastructure}

As big data is not just about hosting data, but about analyzing data the interfaces we provide must encapsulate a rich infrastructure environment that is used by data scientists. This includes the ability to integrate (or plug-in) various compute resources and services to provide the necessary compute power to analyze the data. This includes (a) access to hierarchy of compute resources, from the laptop/desktop, servers, data clusters, and clouds, (b) he ability to integrate special purpose hardware such as GPUs and FPGAs that are used in accelerated analysis of data, and (c) the integration of services including micro services that allow the analysis of the data by delegating them to hosted or dynamically deployed services on the infrastructure of choice.

\subsubsection{Orchestration of Infrastructure and Services}

As part of the use case collection we present in Volume 3, it is obvious that we need to address the mechanism of preparing the preparation of infrastructures suitable for various use cases. As such we are not attempting to deliver a single deployed BDRA, but allow the setup of an infrastructure that satisfies the particular uses case. To achieve this task, we need to provision software tacks and services on infrastructures and orchestrate their deployment, It is not focus of this document to replace existing orchestration software and services, but provide an interface to them to leverage them as part of defining and creating the infrastructure. Various orchestration frameworks and services could therefore be leveraged and work in orchestrated fashion to achieve the goal of preparing an infrastructure suitable for one or more applications.

\subsubsection{Orchestration of Big Data Applications and Experiments}

The creation of the infrastructure suitable for big data applications provides the basic infrastructure. However big data applications may require the creation of sophisticated applications as part of interactive experiments to analyze and probe the data. For this purpose, we need to be able to orchestrate and interact with experiments conducted on the data while assuring reproducibility and correctness of the data. For this purpose, a System Orchestrator (either the Data Scientists or a service acting in behalf of the scientist) uses the BD Application Provider as the command center to orchestrate dataflow from Data Provider, carryout the BD application lifecycle with the help of the BD Framework Provider, and enable Data Consumer to consume Big Data processing results. An interface is needed to describe the interactions and to allow leveraging of experiment management frameworks in scripted fashion. We require a customization of parameters on several levels. On the highest level, we require high level- application motivated parameters to drive the orchestration of the experiment. On lower levels these high-level parameters may drive and create service level agreement augmented specifications and parameters that could even lead to the orchestration of infrastructure and services to satisfy experiment needs.

\subsubsection{Reusability}

The interfaces provided must encourage reusability of the infrastructure, services and experiments described by them. This includes (a) reusability of available analytics packages and services for adoption (b) deployment of customizable analytics tools and services, and (c) operational adjustments that allow the services and infrastructure to be adapted while at the same time allowing for reproducible experiment execution

\subsubsection{Execution Workloads}

One of the important aspects of distributed big data services can be that the data served is simply to big to be moved to a different location. Instead we are in the need of an interface allowing us to describe and package analytics algorithms and potentially also tools as a payload to a data service. This can be best achieved not by sending the detailed execution, but sending an interface description that describes how such an algorithm or tool can be created on the server end and be executed under security considerations integrated with authentication and authorization in mind.

\subsubsection{Security and Privacy Fabric Requirements}

\TODO{Subsection Scope: Discussion of high-level requirements of the interface approach for the Security and Privacy Fabric.}

\subsubsection{System Orchestration Requirement}

\TODO{Subsection Scope: Discussion of high-level requirements of the interface approach for the System Orchestrator.}

\subsubsection{Application Providers Requirements}

\TODO{Subsection Scope: Discussion of high-level requirements of the interface approach for the Application Provider.}

\subsection{Component Specific Interface Requirements}

In this section, we summarize a set of requirements for the interface of a particular component in the NBDRA. The components are listed in Figure 1 and addressed in each of the subsections as part of Section 2.2 of this document. The five main functional components of the NBDRA represent the different technical roles within a Big Data system. The functional components are listed below and discussed in subsequent subsections.
System Orchestrator: Defines and integrates the required data application activities into an operational vertical system;
Big Data Application Provider: Executes a data life cycle to meet security and privacy requirements as well as System Orchestrator-defined requirements;
Data Provider: Introduces new data or information feeds into the Big Data system;
Big Data Framework Provider: Establishes a computing framework in which to execute certain transformation applications while protecting the privacy and integrity of data; and
Data Consumer: Includes end users or other systems that use the results of the Big Data Application Provider.

\subsubsection{System Orchestrator Interface Requirement}

The System Orchestrator role includes defining and integrating the required data application activities into an operational vertical system. Typically, the System Orchestrator involves a collection of more specific roles, performed by one or more actors, which manage and orchestrate the operation of the Big Data system. These actors may be human components, software components, or some combination of the two. The function of the System Orchestrator is to configure and manage the other components of the Big Data architecture to implement one or more workloads that the architecture is designed to execute. The workloads managed by the System Orchestrator may be assigning/provisioning framework components to individual physical or virtual nodes at the lower level, or providing a graphical user interface that supports the specification of workflows linking together multiple applications and components at the higher level. The System Orchestrator may also, through the Management Fabric, monitor the workloads and system to confirm that specific quality of service requirements are met for each workload, and may actually elastically assign and provision additional physical or virtual resources to meet workload requirements resulting from changes/surges in the data or number of users/transactions.
The interface to the system orchestrator must be capable of specifying the task of orchestration the deployment, configuration, and the execution of applications within the NBDRA. A simple vendor neutral specification to coordinate the various parts either as simple parallel language tasks or as a workflow specification is needed to facilitate the overall coordination. Integration of existing tools and services into the orchestrator as extensible interface is desirable.

\subsubsection{Data Provider Interface Requirement}

The Data Provider role introduces new data or information feeds into the Big Data system for discovery, access, and transformation by the Big Data system. New data feeds are distinct from the data already in use by the system and residing in the various system repositories. Similar technologies can be used to access both new data feeds and existing data. The Data Provider actors can be anything from a sensor, to a human inputting data manually, to another Big Data system.
Interfaces for data providers must be able to specify a data provider so it can be located by a data consumer. It also must include enough details to identify the services offered so they can be pragmatically reused by consumers. Interfaces to describe pipes and filters must be addressed.

\subsubsection{Data Consumer Interface Requirement}

Similar to the Data Provider, the role of Data Consumer within the NBDRA can be an actual end user or another system. In many ways, this role is the mirror image of the Data Provider, with the entire Big Data framework appearing like a Data Provider to the Data Consumer. The activities associated with the Data Consumer role include (a) Search and Retrieve (b) Download (c) Analyze Locally (d) Reporting (d) Visualization (e) Data to Use for Their Own Processes. The interface for the data consumer must be able to describe the consuming services and how they retrieve information or leverage data consumers.

\subsubsection{Big Data Application Interface Provide}

The Big Data Application Provider role executes a specific set of operations along the data life cycle to meet the requirements established by the System Orchestrator, as well as meeting security and privacy requirements. The Big Data Application Provider is the architecture component that encapsulates the business logic and functionality to be executed by the architecture. 
The interfaces to describe big data applications include interfaces for the various subcomponents including collections, preparation/curation, analytics, visualization, and access. Some if the interfaces used in these components can be reused from other interfaces introduced in other sections of this document. Where appropriate we will identify application specific interfaces and provide examples of them while focusing on a use case as identified in Volume 3 of this series.

\paragraph{Collection}

In general, the collection activity of the Big Data Application Provider handles the interface with the Data Provider. This may be a general service, such as a file server or web server configured by the System Orchestrator to accept or perform specific collections of data, or it may be an application-specific service designed to pull data or receive pushes of data from the Data Provider. Since this activity is receiving data at a minimum, it must store/buffer the received data until it is persisted through the Big Data Framework Provider. This persistence need not be to physical media but may simply be to an in-memory queue or other service provided by the processing frameworks of the Big Data Framework Provider. The collection activity is likely where the extraction portion of the Extract, Transform, Load (ETL)/Extract, Load, Transform (ELT) cycle is performed. At the initial collection stage, sets of data (e.g., data records) of similar structure are collected (and combined), resulting in uniform security, policy, and other considerations. Initial metadata is created (e.g., subjects with keys are identified) to facilitate subsequent aggregation or look-up methods.

\paragraph{Preparation}

The preparation activity is where the transformation portion of the ETL/ELT cycle is likely performed, although analytics activity will also likely perform advanced parts of the transformation. Tasks performed by this activity could include data validation (e.g., checksums/hashes, format checks), cleansing (e.g., eliminating bad records/fields), outlier removal, standardization, reformatting, or encapsulating. This activity is also where source data will frequently be persisted to archive storage in the Big Data Framework Provider and provenance data will be verified or attached/associated. Verification or attachment may include optimization of data through manipulations (e.g., deduplication) and indexing to optimize the analytics process. This activity may also aggregate data from different Data Providers, leveraging metadata keys to create an expanded and enhanced data set.

\paragraph{Analytics}

The analytics activity of the Big Data Application Provider includes the encoding of the low-level business logic of the Big Data system (with higher-level business process logic being encoded by the System Orchestrator). The activity implements the techniques to extract knowledge from the data based on the requirements of the vertical application. The requirements specify the data processing algorithms for processing the data to produce new insights that will address the technical goal. The analytics activity will leverage the processing frameworks to implement the associated logic. This typically involves the activity providing software that implements the analytic logic to the batch and/or streaming elements of the processing framework for execution. The messaging/communication framework of the Big Data Framework Provider may be used to pass data or control functions to the application logic running in the processing frameworks. The analytic logic may be broken up into multiple modules to be executed by the processing frameworks which communicate, through the messaging/communication framework, with each other and other functions instantiated by the Big Data Application Provider.

\paragraph{Visualization}

The visualization activity of the Big Data Application Provider prepares elements of the processed data and the output of the analytic activity for presentation to the Data Consumer. The objective of this activity is to format and present data in such a way as to optimally communicate meaning and knowledge. The visualization preparation may involve producing a text-based report or rendering the analytic results as some form of graphic. The resulting output may be a static visualization and may simply be stored through the Big Data Framework Provider for later access. However, the visualization activity frequently interacts with the access activity, the analytics activity, and the Big Data Framework Provider (processing and platform) to provide interactive visualization of the data to the Data Consumer based on parameters provided to the access activity by the Data Consumer. The visualization activity may be completely application-implemented, leverage one or more application libraries, or may use specialized visualization processing frameworks within the Big Data Framework Provider. 

\paragraph{Access}

The access activity within the Big Data Application Provider is focused on the communication/interaction with the Data Consumer. Similar to the collection activity, the access activity may be a generic service such as a web server or application server that is configured by the System Orchestrator to handle specific requests from the Data Consumer. This activity would interface with the visualization and analytic activities to respond to requests from the Data Consumer (who may be a person) and uses the processing and platform frameworks to retrieve data to respond to Data Consumer requests. In addition, the access activity confirms that descriptive and administrative metadata and metadata schemes are captured and maintained for access by the Data Consumer and as data is transferred to the Data Consumer. The interface with the Data Consumer may be synchronous or asynchronous in nature and may use a pull or push paradigm for data transfer. 

\paragraph{Big Data Provider Framework Interface Requirements}

Data for Big Data applications are delivered through data providers. They can be either local providers contributed by a user or distributed data providers that refer to data on the internet.  We must be able to provide the following functionality (1) interfaces to files (2) interfaces ti virtual data directories (3) interfaces ti data streams (4) and interfaces to data filters.

\paragraph{Infrastructures Interface Requirements}

This Big Data Framework Provider element provides all of the resources necessary to host/run the activities of the other components of the Big Data system. Typically, these resources consist of some combination of physical resources, which may host/support similar virtual resources.
As part of the NBDRA we need interfaces that can be used to deal with the underlying infrastructure to address networking, computing, and storage

\paragraph{Platforms Interface Requirements}

As part of the NBDRA platforms we need interfaces that can address platform needs and services for data organization, data distribution, indexed storage, and file systems.

\paragraph{Processing Interface Requirements}

The processing frameworks for Big Data provide the necessary infrastructure software to support implementation of applications that can deal with the volume, velocity, variety, and variability of data. Processing frameworks define how the computation and processing of the data is organized. Big Data applications rely on various platforms and technologies to meet the challenges of scalable data analytics and operation. 
We need to be able to interface easily with computing services that offer specific analytics services, batch processing capabilities, interactive analysis, and data streaming.

\paragraph{Crosscutting Interface Requirements}

A number of crosscutting interface requirements within the NBDRA provider frameworks include messaging, communication, and resource management. Often these eservices may actually be hidden from explicit interface use as they are part of larger systems that expose higher level functionality through their interfaces. However, it may be needed to expose such interfaces also on a lower level in case finer grained control is needed. We will identify the need for such crosscutting interface requirements form Volume 3 of this series.

\paragraph{Messaging/Communications Frameworks}

Messaging and communications frameworks have their roots in the High Performance Computing (HPC) environments long popular in the scientific and research communities. Messaging/Communications Frameworks were developed to provide APIs for the reliable queuing, transmission, and receipt of data

\paragraph{Resource Management Framework}

As Big Data systems have evolved and become more complex, and as businesses work to leverage limited computation and storage resources to address a broader range of applications and business challenges, the requirement to effectively manage those resources has grown significantly. While tools for resource management and “elastic computing” have expanded and matured in response to the needs of cloud providers and virtualization technologies, Big Data introduces unique requirements for these tools. However, Big Data frameworks tend to fall more into a distributed computing paradigm, which presents additional challenges. 

\subsubsection{BD Application Provider to Framework Provider
  Interface}

The Big Data Framework Provider typically consists of one or more hierarchically organized instances of the components in the NBDRA IT value chain (Figure 2). There is no requirement that all instances at a given level in the hierarchy be of the same technology. In fact, most Big Data implementations are hybrids that combine multiple technology approaches in order to provide flexibility or meet the complete range of requirements, which are driven from the Big Data Application Provider. 


	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Specification Paradigm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this document we summarize elementary objects that are important to
for the NBDRA.

\subsection{Lessons Learned}

\TODO{TBD}

\subsection{Hybrid Cloud}

\TODO{TBD}

\subsection{Design by Example}

To accelerate discussion among the team we use an approach to define
objects and its interfaces by example. These examples are than taken
in a later version of the document and a schema is generated from
it. The schema will be added in its complete form to the appendix
\ref{a:schema}. While focusing first on examples it allows us to speed
up our design and simplifies discussions of the objects and interfaces
eliminating getting lost in complex syntactical specifications. The
process and specifications used in this document will also allow us to
automatically create a implementation of the objects that can be
integrated into a reference architecture as provided by for example
the cloudmesh client and rest project \cite{www-cloudmesh-client}.

An example object will demonstrate our approach. The following object
defines a JSON object representing a user. 


\codeFromJson{json}{profile.json}{User profile}{s:profile}

 
Such an object can be transformed to a schema specification 
while introspecting the types of the original example. The resulting
schema object follows the Cerberus \cite{www-cerberus} specification and looks
for our object as follows:

\begin{Verbatim}
profile = {  
   'description': { ’type’: ’string’},  
   ’email’: { ’type’: ’email’ },  
   ’firstname’: { ’type’: ’string’},  
   ’lastname’: { ’type’: ’string’ },   
   ’username’: { ’type’: ‘string’ } 
}  
\end{Verbatim}

As mentioned before, the Appendix\ref{a:schema} will list the schema
that is automatically created from the definitions.

\subsection{Tools to Create the Specifications}

The tools to create the schema and object are all available opensource
and are hosted on github. It includes the following repositories:

\begin{description}
\item[cloudmesh.common] ~\\
  \url{https://github.com/cloudmesh/cloudmesh.common}
\item[cloudmesh.cmd5] ~\\
  \url{https://github.com/cloudmesh/cloudmesh.cmd5}
\item[cloudmesh.rest] ~\\
  \url{https://github.com/cloudmesh/cloudmesh.rest}
\item[cloudmesh/evegenie] ~\\
  \url{https://github.com/cloudmesh/evegenie}
\end{description}

\subsection{Installation of the Tools}

The current best way to install the tools is from source. A convenient
shell script conducting the install is located at:

TBD

Once we have stabilized the code the package will be available from
pypi and can be installed as follows:

\begin{Verbatim}
pip install cloudmesh.rest
pip install cloudmesh.evengine
\end{Verbatim} 

\subsection{Document Creation}

It is assumed that you have installed all the tools. TO create the
document you can simply do

\begin{Verbatim}
git clone https://github.com/cloudmesh/cloudmesh.rest
cd cloudmesh.rest/docs
make
\end{Verbatim}

This will produce in that directory a file called object.pdf
containing this document.

\subsection{Conversion to Word}

We found that it is inconvenient for the developers to maintain this
document in Microsoft Word as typically is done for other
documents. This is because the majority of the information contains
specifications that are directly integrated in a reference
implementation, as well as that the current majority of contributors
are developers. We would hope that editorial staff provides direct
help to improve this document, which even can be done through the
github Web interface and does not require any access either to the
tools mentioned above or the availability of \LaTeX.

The files are located at:

\begin{itemize}
\item \url{https://github.com/cloudmesh/cloudmesh.rest/tree/master/docs}
\end{itemize}

\subsection{Interface Compliancy}

Due to the extensibility of our interfaces it is important to
introduce a terminology that allows us to define interface
compliancy. We define it as follows

\begin{description}

\item[Full Compliance:] These are reference implementations that
  provide full compliance to the objects defined in this document. A
  version number will be added to assure the snapshot in time of the
  objects is associated with the version. This reference
  implementation will implement all objects.

\item[Partially Compliance:] These are reference implementations that
  provide partial compliance to the objects defined in this
  document. A version number will be added to assure the snapshot in
  time of the objects is associated with the version. This reference
  implementation will implement a partial list of the objects. A
  document is accompanied that lists all objects defined, but also
  lists the objects that are not defined by the reference
  architecture.

\item[Full and extended Compliance:] These are interfaces that in
  addition to the full compliance also introduce additional interfaces
  and extend them.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Specification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{User and Profile}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In a multiuser environment we need a simple mechanism of associating
objects and data to a particular person or group. While we do not want
to replace with our efforts more elaborate solutions such as proposed
by eduPerson \cite{www-eduperson} or others, we need a very simple way of distinguishing
users. Therefore we have introduced a number of simple objects
including a profile and a user.

\subsubsection{Profile}

A profile is simple the most elementary information to distinguish a
user profile. It contains name and e-mail information. It may have an
optional uuid and/or use a unique e-mail to distinguish a
user. 

\TODO{what does the ``context'' represent? What are possible values?
  How do those values alter the interpretation of a profile?}


\codeFromJson{json}{profile.json}{User profile}{s:profile}

\subsubsection{User}

In contrast to the profile a user contains additional attributs that
define the role of the user within the system.

\TODO{There's redundancy in the definition of Profile and User, namely
  everything except ``roles''. I don't think the current definitions
  clearly illustrate what each is supposed to represent and how they
  fit together in the system.}

\codeFromJson{json}{user.json}{user}{s:user}

\subsubsection{Organization}

An important concept in many applications is the management of a roup
of users in a virtual organization. This can be achieved through two
concepts. First, it can be achieved while useing the profile and user
resources itself as they contain the ability to manage multiple users
as part of the REST interface. The second concept is to create a
virtual organization that lists all users of this virtual
organization. The third concept is to introduce groups and roles
either as part of the user definition or as part of a simple list
similar to the organization


\codeFromJson{json}{organization.json}{user}{s:organization}

\subsubsection{Group/Role}

A group contains a number of users. It is used to manage authorized
services.

\TODO{The examples objects for Organization, Group, and Role should
  clearly illustrate the differences. Right now it is a bit unclear.}

\codeFromJson{json}{group.json}{group}{s:group}

A role is a further refinement of a group. Group members can have
specific roles. A good example is that ability to formulate a group of
users that have access to a repository. However the role defines more
specifically read and write privileges to the data within the repository.

\codeFromJson{json}{role.json}{role}{s:role}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Data for Big Data applications are delivered through data
providers. They can be either local providers contributed by a user or
distributed data providers that refer to data on the internet. At this
time we focus on an elementary set of abstractions related to data
providers that offer us to utilize variables, files, virtual data
directories, data streams, and data filters.

\begin{description}
\item[Variables] are used to hold specific contents that is associated
  in programming language as a variable. A variable has a name, value
  and type.

\item[Default] is a special type of variable that allows adding of a
  context. Defaults can than created for different contexts.

\item[Files] are used to represent information collected within the
  context of classical files in an operating system. \TODO{I don't
    think this is very clear. Elaborate with examples?}.

\item[Streams] are services that offer the consumer a stream of
  data. Streams may allow the initiation of filters to reduce the
  amount of data requested by the consumer.  Stream Filters operate in
  streams or on files converting them to streams.
  \TODO{What are the semantics of streams?}

\item[Batch Filters] operate on streams and on files while working in
  the background and delivering as output Files. \TODO{Whats the
    difference between Batch Filters and Stream Filters mentioned in
    Streams?}

\item[Virtual directories] and non-virtual directories are collection
  of files that organize them. For our initial purpose the distinction
  between virtual and non-virtual directories is non-essential and we
  will focus on abstracting all directories to be virtual. This could
  mean that the files are physically hosted on different
  disks. However, it is important to note that virtual data
  directories can hold more than files, they can also contain data
  streams and data filters. \TODO{Do we have examples of what this
    would look like?}

\end{description}

\subsubsection{Var}

Variables are used to store a simple values. Each variable can have a
type. The variable value format is defined as string to allow maximal
probability. The type of the value is also provided.

\codeFromJson{json}{var.json}{var}{s:var}

\subsubsection{Default}

A default is a special variable that has a context associated with
it. This allows one to define values that can be easily retrieved
based on its context. A good example for a default would be the image
name for a cloud where the context is defined by the cloud name.

\codeFromJson{json}{default.json}{default}{s:default}


\begin{figure}[!h]
\centering
\includegraphics[width=0.5\columnwidth]{images/uml/boot.pdf}
\caption{Booting a virtual machine from defaults}\label{F:uml-boot}
\end{figure}


\subsubsection{File}

A file is a computer resource allowing to store data that is being
processed. The interface to a file provides the mechanism to
appropriately locate a file in a distributed system. Identification
include the name, and andpoint, the checksum and the size. Additional
parameters such as the lasst access time could be stored also. As such
the Interface only describes the location of the file 

The \textit{file} object has \textit{name}, \textit{endpoint} (location), \textit{size}
in GB, MB, Byte, \textit{checksum} for integrity check, and last
\textit{accessed} timestamp. 

\codeFromJson{json}{file.json}{file}{s:file}

\subsubsection{File Alias}

A file could have one alias or even multiple ones. The reason for an alias is that a file may have a complex name but a user may want to refer to that file in a name space that is suitable for the users application.

\codeFromJson{json}{file_alias.json}{file alias}{s:file-alias}

\subsubsection{Replica}

In many distributed systems, it is of importance that a file can be
replicated among different systems in order to provide faster
access. It is important to provide a mechanism that allows to trace
the pedigree of the file while pointing to its original source. The need for the replicat is  

\TODO{We need to describe why a Replica is different from a File
  object.}

\codeFromJson{json}{replica.json}{replica}{s:replica}


\subsubsection{Virtual Directory}

A collection of files or replicas. A virtual directory can contain an
number of entities cincluding files, streams, and other virtual
directories as part of a collection. The element in the collection can
either be defined by uuid or by name. 

\codeFromJson{json}{virtual_directory.json}{virtual
  directory}{s:virtual-directory}

\subsubsection{Database}

A \textit{database} could have a name, an \textit{endpoint} (e.g., host:port),
and protocol used (e.g., SQL, mongo, etc.).

\codeFromJson{json}{database.json}{database}{s:database}

\subsubsection{Stream} 

A stream proveds a stream of data while providing information about
rate and number of items exchanged while issuing requests to the
stream. A stream my return data items in a specific fromat that is
defined by the stream. 

\codeFromJson{json}{stream.json}{stream}{s:stream}

Examples for streams could be a stream of random numbers but could
also include more complex formats such as the retrieval of data
records. 

Services can subscribe, unsubscribe from a stream, while also applying
filters to the subscribed stream.

\codeFromJson{json}{filter.json}{filter}{s:filter}

\TODO{Filter needs to be refined}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{IaaS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this subsection we are defining resources related to Infrastructure as
a Service frameworks. This includes specific objects useful for
OpenStack, Azure, and AWS, as well as others.

% ----------------------------------------------------------------------
\subsubsection{Openstack}
% ----------------------------------------------------------------------

\paragraph{Openstack Flavor}

\codeFromJson{json}{openstack_flavor.json}{openstack flavor}{s:openstack-flavor}

\paragraph{Openstack Image}

\codeFromJson{json}{openstack_image.json}{openstack
  image}{s:openstack-image}

\paragraph{Openstack Vm}

\codeFromJson{json}{openstack_vm.json}{openstack
  vm}{s:openstack-vm}

% ----------------------------------------------------------------------
\subsubsection{Azure}
% ----------------------------------------------------------------------

\paragraph{Azure Size}

The size description of an azure vm

\codeFromJson{json}{azure-size.json}{azure-size}{s:azure-size}


\paragraph{Azure Image}

\codeFromJson{json}{azure-image.json}{azure-image}{s:azure-image}

\paragraph{Azure Vm}

An Azure virtual machine
\codeFromJson{json}{azure-vm.json}{azure-vm}{s:azure-vm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{HPC}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Batch Job}

\codeFromJson{json}{batchjob.json}{batchjob}{s:batchjob}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Virtual Cluster}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Cluster}

The cluster object has name, label, endpoint and provider. The
\textit{endpoint} defines....  The \textit{provider} defines the
nature of the cluster, e.g., its a virtual cluster on an openstack
cloud, or from AWS, or a bare-metal cluster.


\codeFromJson{json}{cluster.json}{cluster}{s:cluster}

\subsubsection{New Cluster}

\codeFromJson{json}{cluster_new.json}{cluster}{s:cluster}
\
\subsubsection{Compute Resource}

An important concept for big data analysis it the representation of a
compute resource on which we execute the analysis. We define a compute
resource by name and by endpoint. A compute resource is an abstract
concept and can be instantiated through virtual machines, containers,
or bare metal resources. This is defined by the “kind” of the compute
resource 

\textit{compute\_resource} object has attribute \textit{endpoint} which
specifies ... The \textit{kind} could be \textit{baremetal} or \textit{VC}.

\codeFromJson{json}{compute_resource.json}{compute resource}{s:compute-resource}

\subsubsection{Computer}

This defines a \textit{computer} object. A computer has name, label,
IP address. It also listed the relevant specs such as memory, disk
size, etc.

\codeFromJson{json}{computer.json}{computer}{s:computer}


\subsubsection{Compute Node}

A node is composed of multiple components:

\begin{enumerate}
\item Metadata such as the \verb|name| or \verb|owner|.
\item Physical properties such as \verb|cores| or \verb|memory|.
\item Configuration guidance such as \verb|create_external_ip|,
  \verb|security_groups|, or \verb|users|.
\end{enumerate}

The metadata is associated with the node on the provider end (if
supported) as well as in the database. Certain parts of the metadata
(such as \verb|owner|) can be used to implement access
control. Physical properties are relevant for the initial allocation
of the node. Other configuration parameters control and further
provisioning.

In the above, after allocation, the node is configured with a user
called \verb|hello| who is part of the \verb|wheel| group whose
account can be accessed with several SSH identities whose public keys
are provided (in \verb|authorized_keys|).

Additionally, three ssh keys are generated on the node for the
\verb|hello| user. The first uses the \verb|ed25519| cryptographic
method with a password read in from a GPG-encrypted file on the
Command and Control node. The second is a 4098-bit RSA key also
password-protected from the GPG-encrypted file. The third key is
copied to the remote node from an encrypted file on the Command and
Control node.

This definition also provides a security group to control access to
the node from the wide-area-network. In this case all ingress and
egress TCP and UDP traffic is allowed provided they are to ports 22
(SSH), 443 (SSL), and 80 and 8080 (web).


\codeFromJson{json}{node.json}{node}{s:node}



\subsubsection{Virtual Cluster}

A virtual cluster is an agglomeration of virtual compute nodes that
constitute the cluster. Nodes can be assembled to be baremetal,
virtual machines, and containers. A virtual cluster contains a number
of virtual compute nodes.  
 
\codeFromJson{json}{virtual_cluster.json}{virtual
  cluster}{s:virtual-cluster}

\subsubsection{Virtual Compute node}

\codeFromJson{json}{virtual_compute_node.json}{virtual
  compute node}{s:virtual-compute-node}

\subsubsection{Virtual Machine}

Virtual Machine 
Virtual machines are an emulation of a computer system. We are maintaining a very basic set of information. It is expected that through the endpoint the virtual machine can be introspected and more detailed information can be retrieved. 

\codeFromJson{json}{virtual_machine.json}{virtual machine}{s:virtual-machine}

\subsubsection{Mesos}

\TODO{Refine}

\codeFromJson{json}{mesos.json}{mesos}{s:mesos}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Containers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Container}

This defines {\em container} object.

\codeFromJson{json}{container.json}{container}{s:container}

\subsubsection{Kubernetes}

\TODO{REFINE}

\codeFromJson{json}{kubernetes.json}{kubernetes}{s:kubernetes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deployment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Deployment}

A \textit{deployment} consists of the resource \- \textit{cluster},
the location \- \textit{provider}, e.g., AWS, OpenStack, etc., and
software \textit{stack} to be deployed (e.g., hadoop, spark).

\codeFromJson{json}{deployment.json}{deployment}{s:deployment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mapreduce}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Mapreduce}

The \textit{mapreduce} deployment has as inputs parameters defining
the applied function and the input data.  Both function and data
objects define a ``source'' parameter, which specify the location it
is retrieved from. For instance, the ``file://'' URI indicates sending
a directory structure from the local file system where the ``ftp://''
indicates that the data should be fetched from a FTP resource. It is
the framework's responsibility to materialize and instantiation of the
desired environment along with the function and data.

\codeFromJson{json}{mapreduce.json}{mapreduce}{s:mapreduce}

Additional parameters include the ``fault\_tolerant'' and ``backend''
parameters.  The former flag indicates if the \textit{mapreduce}
deployment should operate in a fault tolerant mode. For instance, in
the case of Hadoop, this may mean configuring automatic failover of
name nodes using Zookeeper.  The ``backend'' parameter accepts an
object describing the system providing the \textit{mapreduce}
workflow.  This may be a native deployment of Hadoop, or a special
instantiation using other frameworks such as Mesos.

A function prototype is defined in Listing~\ref{lst:s:mr.f.prototype}.
Key properties are that functions describe their input parameters and
generated results. For the former, the ``buildInputs'' and
``systemBuildInputs'' respectively describe the objects which should
be evaluated and system packages which should be present before this
function can be installed. The ``eval'' attribute describes how to
apply this function to its input data. Parameters affecting the
evaluation of the function may be passed in as the ``args'' attribute.
The results of the function application can be accessed via the
``outputs'' object, which is a mapping from arbitrary keys
(e.g. ``data'', ``processed'', ``model'') to an object representing
the result.

\codeFromJson{json}{mapreduce_function_prototype.json}{mapreduce function}{s:mr.f.prototype}


Some example functions include the ``NoOp'' function shown in
Listing~\ref{lst:s:mr.f.noop}.  In the case of undefined arguments,
the parameters default to an identity element. In the case of mappings
this is the empty mapping while for lists this is the empty list.

\codeFromJson{json}{mapreduce_function_noop.json}{mapreduce noop}{s:mr.f.noop}


\subsubsection{Hadoop}

A \textit{hadoop} definition defines which \textit{deployer} to be used,
the \textit{parameters} of the deployment, and the system packages as
\textit{requires}. For each requirement, it could have attributes such
as the library origin, version, etc.

\codeFromJson{json}{hadoop.json}{hadoop}{s:hadoop}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Security}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Key}

\codeFromJson{json}{key.json}{key}{s:key}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Microservice}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Microservice}

 
introduce registry we can register many things to it 
latency 
provide example on how to use each of them, not just the object definition example 
 
necessity of local direct attached storage. 
Mimd model to storage  
Kubernetis, mesos can not spin up ?  
Takes time to spin them up and coordinate them. While setting up environment takes more thsn using the microservice, so we must make sure that the micorservices are used sufficiently to offset spinup cost. 
 
limitation of resource capacity such as networking. 
 
Benchmarking to find out thing about service level agreement to access
the 


A system could be composed of from various microservices, and this defines
each of them.

\codeFromJson{json}{microservice.json}{microservice}{s:microservice}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Reservation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\codeFromJson{json}{reservation.json}{reservation}{s:reservation}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Accounting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


As in big data applications and systems considerable amount of
resources are used an accounting system must be present either on the
server side or on the application and user side to allow checking of
balances. Due to the potential heterogeneous nature of the services
used existing accounting frameworks may not be present to dela with
this issue. E.g. we see potentially the use of multiple accounting
systems with different scales of accuracy information feedback
rates. For example, if the existing accounting system informs the user
only hours after she has started a job this could pose a significant
risk because charging is started immediately. While making access to
big data infrastructure and services more simple, the user or
application may underestimate the overall cost projected by the
implementation of the big data reference architecture.

\codeFromJson{json}{accounting.json}{accounting}{s:accounting}

\codeFromJson{json}{account.json}{account}{s:account}

\paragraph{Usecase: Accounting Service}

Figure \ref{F:create resource} depicts a possible accounting service that allows an administrator to register a variety of resources to an account for a user. The services that are than invoked by the user can than consume the resource and are charged accordingly.

\begin{figure}[!h]
\includegraphics[width=\columnwidth]{images/dot/account.pdf}
\caption{Create Resource}\label{F:createresource}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\columnwidth]{images/uml/account.pdf}
\caption{Accounting}\label{F:uml-accounting}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We are looking for volunteers to contribute here.


%\bibliography{references}

\newpage

\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Schema Command}


\codeFromFile{bash}{schema-man.tex}{man page}{s:manpage}


\subsection{Schema}\label{a:schema}

TBD

\codeFromJson{python}{../settings.settings.py}{schema}{s:schema}

\subsection{Contributing}

We invite you to contribute to this paper and its discussion to
improve it. Improvements can be done with pull requests. We suggest
you do {\em small} individual changes to a single subsection and object
rather than large changes as this allows us to integrate the changes
individually and comment on your contribution via github.

Once contributed we will appropriately acknowledge you either as
contributor or author. Please discuss with us how we best acknowledge
you.

\subsection{Using the Cloudmesh REST Service} 

Components are written as YAML markup in files in the
\verb+resources/samples+ directory.

For example:

\codeFromJson{json}{profile.json}{profile}{s:profile}

\subsubsection{Element Definition}

Each resource should have a \verb+description+ entry to act as
documentation. The documentation should be formated as
reStructuredText. For example:

\subsubsection{Yaml}

\begin{Verbatim}
entry = yaml.read('''
profile:
  description: |
    A user profile that specifies general information 
    about the user
  email: laszewski@gmail.com, required
  firtsname: Gregor, required
  lastname: von Laszewski, required
  height: 180
'''}
\end{Verbatim}

\subsubsection{Cerberus}

\begin{Verbatim}
schema = {
'profile': {
  'description': {'type': 'string'}
  'email':       {'type': 'string', 'required': True}
  'firtsname':   {'type': 'string', 'required': True}
  'lastname':    {'type': 'string', 'required': True}
  'height':      {'type': 'float'}
}
\end{Verbatim}

\subsection{Mongoengine}

\begin{Verbatim}
class profile(Document):
    description = StringField()
    email = EmailField(required=True)
    firstname = StringField(required=True)
    lastname = StringField(required=True)
    height = FloatField(max_length=50)
\end{Verbatim}

\subsection{Cloudmesh Notation}

\begin{Verbatim}
profile:
    description: string
    email: email, required
    firstname: string, required
    lastname: string, required
    height: flat, max=10
\end{Verbatim}

\begin{Verbatim}
proposed command

cms schema FILENAME --format=mongo -o OUTPUT
cms schema FILENAME --format=cerberus -o OUTPUT
cms schema FILENAME --format=yaml -o OUTPUT

  reads FILENAME in cloudmesh notation and returns format


cms schema FILENAME --input=evegenie -o OUTPUT
   reads evegene example and create settings for eve
\end{Verbatim}


\subsubsection{Defining Elements for the REST Service}

To manage a large number of elements defined in our REST service
easily, we manage them though definitions in yaml files. To generate
the appropriate settings file for the rest service, we can use the
following command:

\begin{verbatim}
cms admin elements <directory> <out.json>
\end{verbatim}

where

\begin{itemize}
\item \verb+<directory>+: directory where the YAML definitions reside
\item \verb+<out.json>+: path to the combined definition
\end{itemize}

For example, to generate a file called all.json that integrates all
yml objects defined in the directory \verb+resources/samples+ you can
use the following command:

\begin{verbatim}
cms elements resources/samples all.json
\end{verbatim}

\subsubsection{DOIT}


cms schema spec2tex resources/specification resources/tex

\subsubsection{Generating service}

With evegenie installed, the generated JSON file from the above step
is processed to create the stub REST service definitions.


\subsection{ABC}
{\bf README.rst}
\input{includes/readme}
classes lessons rest.rst
\input{includes/rest}
classes lesson python cmd5.rst
\input{includes/python-cmd5}

\subsection{Acronyms and Terms}

The following acronyms and terms are used in the paper

\begin{description}[leftmargin=8em,style=nextline]
\item[ACID] 	       Atomicity, Consistency, Isolation, Durability
\item[API] 	       Application Programming Interface
\item[ASCII] 	       American Standard Code for Information Interchange 
\item[BASE] 	       Basically Available, Soft state, Eventual consistency
\item[Container]       \TODO{define}
\item[Cloud]           \TODO{define}
\item[DevOps] 	       A clipped compound of {\em software DEVelopment} and
                       {\em information technology OPerationS}
\item[Deployment]      \TODO{define}
\item[HTTP] 	       HyperText Transfer Protocol HTTPS HTTP Secure
\item[Hybrid Cloud]    \TODO{define}
\item[IaaS] 	       Infrastructure as a Service SaaS Software as a Service
\item[ITL] 	       Information Technology Laboratory
\item[Microservice]    \TODO{define}
\item[NBD-PWG]	       NIST Big Data Public Working Group
\item[NBDRA] 	       NIST Big Data Reference Architecture 
\item[NBDRAI] 	       NIST Big Data Reference Architecture Interface
\item[NIST] 	       Big Data Interoperability Framework: Volume 8, Reference Architecture Interface
\item[NIST] 	       National Institute of Standards
\item[OS] 	       Operating System
\item[REST] 	       REpresentational State Transfer
\item[Replica]         \TODO{define}
\item[Serverless Computing] \TODO{define}
\item[Software Stack]  \TODO{define}
\item[Virtual Filesysyem] \TODO{define}
\item[Virtual Machine] \TODO{define}
\item[Virtual Cluster] \TODO{define}
\item[Workflow]        \TODO{define}
\item[WWW] 	       World Wide Web
\end{description}



\end{document}



